{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a5c7e-c6d9-4312-a74f-ffec37fff35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f12a53-7371-4035-9fff-cb0d70b0ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "\n",
    "Boosting is a resilient method that curbs over-fitting easily. One disadvantage of boosting is that it is sensitive to outliers since every classifier is obliged to fix the errors in the predecessors. Thus, the method is too dependent on outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a8c22-eae6-4b6d-b0f9-9bc2e0424201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "Boosting algorithms combine multiple weak learners in a sequential method, which iteratively improves observations. This approach helps to reduce high bias that is common in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39436e86-e1fc-426f-b4c5-9783ca006caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "\n",
    "Boosting algorithms are ensemble learning techniques that combine the predictions of multiple weak learners (typically decision trees) to create a strong learner. Popular boosting algorithms include AdaBoost, Gradient Boosting Machine (GBM), XGBoost, LightGBM, and CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95981031-0272-44cf-be32-8036cba4e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "\n",
    "Boosting algorithms, such as AdaBoost, Gradient Boosting, and XGBoost, typically have several common parameters that influence their behavior and performance. Here are some of the common parameters:\n",
    "\n",
    "Number of Estimators (n_estimators): This parameter determines the number of boosting stages or trees to be built. Increasing the number of estimators generally improves the model's performance but also increases computation time.\n",
    "\n",
    "Learning Rate (or Shrinkage) (learning_rate): Controls the contribution of each tree to the ensemble. Lower values require more trees to achieve the same performance but can generalize better. It acts as a regularization parameter.\n",
    "\n",
    "Tree-specific Parameters (max_depth, min_samples_split, min_samples_leaf, max_features): These parameters control the structure, size, and complexity of individual trees in the ensemble. They affect how each base learner (tree) is constructed.\n",
    "\n",
    "Loss Function (loss): Specifies the loss function to be optimized during training. Different boosting algorithms may support different loss functions suited to specific tasks, such as regression or classification.\n",
    "\n",
    "Subsample (subsample): Specifies the fraction of samples used for fitting the individual base learners. It can be used to introduce stochasticity into the training process, improving generalization.\n",
    "\n",
    "Warm Start (warm_start): Allows for incremental training. If set to True, the model starts training from the previous call to fit, which can be useful for adding more estimators to an already trained model.\n",
    "\n",
    "Early Stopping (early_stopping_rounds): In some implementations like XGBoost, allows the training to stop early if the validation score stops improving for a specified number of rounds. Helps in avoiding overfitting and speeding up training.\n",
    "\n",
    "Regularization Parameters (reg_alpha, reg_lambda): Control L1 and L2 regularization applied to the base learners (trees) to prevent overfitting by penalizing large coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915f0d9-f4e5-4fb9-987e-ecc61c94488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "\n",
    "Bias is the presence of uncertainty or inaccuracy in machine learning results. Boosting algorithms combine multiple weak learners in a sequential method, which iteratively improves observations. This approach helps to reduce high bias that is common in machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bb517-33e2-4d8c-b7a6-bd018c0ba4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "\n",
    "Adaboost is an ensemble learning technique used to improve the predictive accuracy of any given model by combining multiple “weak” learners. Adaboost works by weighting incorrectly classified instances more heavily so that the subsequent weak learners focus more on the difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93add31e-c7b2-4388-887f-204fe28c5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8 \n",
    "\n",
    "For AdaBoost, the basis functions are the classifiers Gm, and they produce the output of either — 1 or + 1. Using the exponential loss function, we now must solve at every step that we essentially minimise over β and G the sum over the exponential loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e20a8-99c4-4a6b-afbf-08f626b467f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
